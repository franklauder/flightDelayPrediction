---
title: "Flight Delay Prediction"
author: "Frank Laudert"
date: "2025-10-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


For the  first part of our flight arrival project, we cleaned, prepared and explored the combined flight data sets. (https://franklauder.github.io/Flight_Delays/flightAnalysis.html) For the second part of our project, we are ready to move on to predict whether flights will be on time or later. Our focus is on flights arriving late. For predicting flight arrivals, we will use three classifier algorithms (logistic regression, random forest, and gradient boost). The data sets were downloaded from Kaggle. They can be viewed by going to the following link: https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018


# Libraries

```{python}




import pyspark
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.ml.linalg import Vectors

from pyspark.ml.param import Param, Params
from pyspark.sql.functions import col, desc
from pyspark.sql.types import StringType,BooleanType,IntegerType,DoubleType,StructType,StructField,LongType,ShortType

from pyspark.ml.feature import VectorAssembler,MinMaxScaler, StandardScaler

from pyspark.ml import Pipeline

from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics

from pyspark.mllib.stat import Statistics
from pyspark.ml.feature import StringIndexer,OneHotEncoder


from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,GBTClassifier,RandomForestClassifier,LinearSVC


from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator


import pyspark.ml.evaluation as evals

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,CrossValidatorModel

from pyspark.ml.stat import Correlation

from pyspark.sql.functions import *

from pyspark.sql.window import *
from pyspark.sql.functions import row_number

import itertools
from pyspark.sql import SQLContext

from pyspark import SparkContext




```



```{python}




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import seaborn as sns


```


```{python}



from sklearn.metrics import roc_curve, precision_recall_curve, auc


```

```{python}



pd.set_option("display.max_rows", None)

```


```{python}


from pyspark import SparkContext


```


```{python}



print(spark)
print(sc)


```



```{python}



spark.conf.set("spark.sql.adaptive.enabled", "true")


```

# Data

## Data Dictionary


OP_CARRIER-Airline carrier code

DEP_DELAY-Difference in minutes between scheduled and actual departure time. Early departures show negative numbers.

AXI_OUT-The time elapsed between departure from the origin airport gate and wheels off, in minutes.

TAXI_IN-Taxi time from wheels down to arrival at the gate, in minutes. CARRIER_DELAY-Carrier Delay, in Minutes. Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.

WEATHER_DELAY-Weather Delay, in Minutes. Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.

NAS_DELAY-National Air System Delay, in Minutes.Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc. Delays that occur after Actual Gate Out are usually attributed to the NAS and are also reported through OPSNET.

SECURITY_DELAY-Security Delay, in Minutes. Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.

LATE_AIRCRAFT_DELAY-Late Aircraft Delay, in Minutes. Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation.

schDepWhOffDiff-Difference between scheduled departure time and wheels up.

wheOnScArrDiff-Difference between wheels down and scheduled arrival time.

DayOfWeek-1 (Monday) - 7 (Sunday)

Month-1-12

DepartureDelayGroups-Departure Delay intervals, every (15 minutes from <-15 to >180). Negative observations are set to zero.

0 Delay between 0 and 14 minutes 1 Delay between 15 to 29 minutes 2 Delay between 30 to 44 minutes 3 Delay between 45 to 59 minutes 4 Delay between 60 to 74 minutes 5 Delay between 75 to 89 minutes 6 Delay between 90 to 104 minutes 7 Delay between 105 to 119 minutes 8 Delay between 120 to 134 minutes 9 Delay between 135 to 149 minutes 10 Delay between 150 to 164 minutes 11 Delay between 165 to 179 minutes 12 Delay >= 180 minutes

DepTimeBlk-CRS Departure Time Block, Hourly Intervals

1--001-0559 12:00AM to 5:59AM 2--0600-0659 6:00AM to 6:59AM 3--0700-0759 7:00AM to 7:59AM 4--0800-0859 8:00AM to 8:59AM 5--0900-0959 9:00AM to 9:59AM 6--1000-1059 10:00AM to 10:59AM 7--1100-1159 11:00AM to 11:59AM 8--1200-1259 12:00PM to 12:59PM 9--1300-1359 1:00PM to 1:59PM 10--1400-1459 2:00PM to 2:59PM 11--1500-1559 3:00PM to 3:59PM 12--1600-1659 4:00PM to 4:59PM 13--1700-1759 5:00PM to 5:59PM 14--1800-1859 6:00PM to 6:59PM 15--1900-1959 7:00PM to 7:59PM 16--2000-2059 8:00PM to 8:59PM 17--2100-2159 9:00PM to 9:59PM 18--2200-2259 10:00PM to 10:59PM 19--2300-2359 11:00PM to 11:59PM

DistanceGroup-Distance Intervals, every 250 Miles, for Flight Segment

1--Less Than 250 Miles 2--250-499 Miles 3--500-749 Miles 4--750-999 Miles 5--1000-1249 Miles 6--1250-1499 Miles 7--1500-1749 Miles 8--1750-1999 Miles 9--2000-2249 Miles 10--2250-2499 Miles 11--500 Miles and Greater



## Import & Review

We will import our cleaned flight data from its stored location in a Amazon Web Services S3 bucket

```{python}


flight_10=spark.read.format('csv').options(header='true', inferSchema='true').load('s3://databricks-workspacedemo-cloud-storage-bucket/DBWorkData/csv3/flight_10/')


```



```{python}



flight_10.show()


```




```{python}


display(flight_10)


```



```{python}


flight_10.count()


```



```{python}

flight_10.columns


```



```{python}


print(flight_10.dtypes)


```


```{python}

flight_10.printSchema()

```




Our review of the flight_10 dataset reveals it has nineteen columns and a little over six million rows. 

We will transform certain features from numeric data types to categorical data data types.


```{python}


flight_11= flight_10.withColumn("DepTimeBlk",col("DepTimeBlk").cast(StringType())) \
            .withColumn("Month",col("Month").cast(StringType())) \
            .withColumn('DayofMonth',col('DayofMonth').cast(StringType())) \
            .withColumn('DayOfWeek',col('DayOfWeek').cast(StringType())) \
            .withColumn("ArrDel15",col("ArrDel15").cast(StringType())) \
            .withColumn("DepartureDelayGroups",col("DepartureDelayGroups").cast(StringType())) \
            .withColumn("DistanceGroup",col("DepartureDelayGroups").cast(StringType()))


```



```{python}



flight_11.printSchema() 


```


# Feature Selection

## Numeric Features


Correlation will be used to determine if there are mutual relationships between our numeric features.


First, we will extract numeric features from the data set. 

```{python}


numeric_cols = [
    c for c, t in flight_11.dtypes if t.startswith('string') == False
]


```




```{python}



numeric_cols = [
    c for c, t in flight_11.dtypes if t.startswith('string') == False
]

```


```{python}


print(numeric_df.dtypes)


```

Next, the VectorAssembler function will be used to transform the numeric features into a single column (Vector). VectorAssembler will appear again during the building of our pipeline.

```{python}


num_assembler = VectorAssembler(inputCols=numeric_cols,
                                outputCol="num_features")


```



```{python}


df_num_assembler = num_assembler.transform(numeric_df)


```



```{python}


df_num_assembler.show(3)

```


```{python}


or k, v in df_num_assembler.schema["num_features"].metadata["ml_attr"][
        "attrs"].items():
    features_df = pd.DataFrame(v)
column_names = list(features_df['name'])


```




```{python}


spark.createDataFrame(features_df).show(11)


```


```{python}


features_df.info()


```



```{python}


df_vector = df_num_assembler.rdd.map(lambda x: x['num_features'].toArray())


```


```{python}

df_vector.take(4)



```





```{python}


correlation_type = 'pearson'


```


```{python}


matrix = Statistics.corr(df_vector, method=correlation_type)

```



```{python}


corr_df = pd.DataFrame(matrix, columns=column_names, index=column_names)


```




```{python}


final_corr_df = pd.DataFrame(corr_df.abs().unstack().sort_values(kind='quicksort')).reset_index()
final_corr_df.rename({'level_0': 'col1', 'level_1': 'col2', 0: 'correlation_value'}, axis=1, inplace=True)
final_corr_df = final_corr_df[final_corr_df['col1'] != final_corr_df['col2']]


```

We are setting our correlation cutoff at .70. This means if any pair of features show a relationship of .70 or greater than one feature will be dropped from the data set.

```{python}


correlation_cutoff = 0.70 #custom parameter
final_corr_df[final_corr_df['correlation_value'] > correlation_cutoff]


```

Our result came back as an empty data frame. This means there are no features that meet our correlation cutoff. All numeric features will remain in our data set.



## Categorical Features


We will drop the feature 'DayofMonth' from the data set as it duplicates other features. 



```{python}



flight_12=flight_11.drop('DayofMonth')


```




```{python}


flight_12.printSchema()



```

# Pre-processing

## Sampling


```{python}


flight_12.count()


```

```{python}

flight_12_percent=flight_12.groupBy("ArrDel15").count().withColumn("Percent",col("count")/flight_12.count()*100)
flight_12_percent.show()

```

From our inspection of our data earlier, we observed that our data has just over sixty million rows. This size will take large resources and time to complete our classification models. Thus, we will sample our flight data to get a smaller representation that will use less resources and take less time to process.




```{python}

flight_sample = flight_12.sample(False, 0.3, 11)


```

```{python}


flight_sample.count()


```


```{python}


arrDelay_percent=flight_sample.groupBy("ArrDel15").count().withColumn("Percent",col("count")/flight_sample.count()*100)
arrDelay_percent.show()


```


Our sampled data is now just over 18 million rows. Additionally, the target feature's lables (0,1) are the same percentage as the original flight_12 data. 



## Pipeline

A Pipeline is simply a sequence of stages, each of which is either an Estimator or a Transformer. Each stage must be executed in order. An estimator is an object that fits a model based on the input data. Essentially, an estimator is an algorithm which can be fitted on a data frame to produce a transformer. A transformer is an algorithm which can transform one data frame into another data frame. In other words, a transformer takes a data frame with features and produces a data frame with additional features.

### Categorical Features

The first steps are to transform our categorical features. First, we transform our target feature ArrDel15 and rename it to "label" This will is accomplished by using StringIndexer. StringIndexer helps convert categorical string columns in a Data Frame into numerical indices. This conversion is necessary because most machine learning algorithms cannot work directly with string data. StringIndexer converts categorical string columns by assigning a unique index to each distinct string value in the input column and maps it to a new output column of integer indices.



```{python}

renamed=flight_sample.withColumn('label_str',flight_sample['ArrDel15'].cast(StringType()))

indexer=StringIndexer(inputCol='label_str',outputCol='label')


```

The .fit method is used to scan the input column to find unique string values present. The fit method then creates a mapping (think lookup table) where each unique string value is assigned a numerical index. It then returns a StringIndexerModel. 

Once we have our StringIndexer model we use the transform method to apply the learned mapping from String Indexer Model to the original data and create a new column (outputCol in the code). The outputCol contains the numerical indices for each value in the input column. 




```{python}

indexed = indexer.fit(renamed).transform(renamed)


```

```{python}


indexed.printSchema()


```

From the schema we find that the renamed target feature "label" now appears. We no longer require the original target feature name of "ArrDel15" and the "label_str" feature.


```{python}

indexed=indexed.drop("ArrDel15",'label_str')


```


```{python}


indexed.printSchema()


```

Next, predictor features will be transformed by the StringIndexer function. First the categorical columns will be extracted. The following code will pull all string data from the indexed data set, exclude the’ label’ feature, and create a list of categorical columns named categoricalCols.



```{python}


categoricalCols = [field for (field, dataType) in indexed.dtypes 
                   if dataType == "string"
                  and field != 'label']
indexOutputCols = [x + "Index" for x in categoricalCols]

```


```{python}

type(categoricalCols)


```


```{python}

categoricalCols



```




```{python}

stringIndexer = StringIndexer(inputCols=categoricalCols,
                              outputCols=indexOutputCols,
                              handleInvalid="skip")



```



The StringIndexer function takes in as input our categorical columns and returns as output the same categorical columns only now indexed. 



```{python}



indexOutputCols


```




We see that the indexOutPutCols string shows all categorical features as indexed.

Many machine learning algorithms require all the input and output variables to be numeric. For our purposes this requires all strings data types to be transformed into numeric data types. We will accomplish this by Appling one-hot-encoding to a string features. One-hot encoding will create new columns as much as the number of unique instances there are for a feature. If you have a feature 'performance' with three levels ('good', 'fair', 'poor'), the new columns will be filled with 0s and 1s. Our data frame now has three new features, 'performance.good', 'performanc.fair', and 'performance.poor'. This is considered creating dummy features.




```{python}

oheOutputCols = [x + "OHE" for x in categoricalCols]



```


```{python}



oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)


```

We now will extract our numerical columns from the indexed data. 



```{python}



oheEncoder


```

### Numerical Features

Numerical features, excluding the label, will be extracted from the indexed data frame. 





```{python}


numericCols = [
    field for (field, dataType) in indexed.dtypes
    if ((dataType == "int") & (field != "label"))
]



```

### Assemble


Categorcal and Numerical features lists will be combined for one features list.

```{python}


feature_list = oheOutputCols + numericCols



```


VectorAssembler will now be used to to combine the categorical and numeric list into one vector column.


```{python}

assembler = VectorAssembler(inputCols=feature_list, outputCol="features")


```


```{python}

All transformers used will be included in one list called stages.


```

```{python}

stages = [stringIndexer, oheEncoder, assembler]


```

```{python}

selectedCols = ['label', 'features'] + feature_list


```

Now that our pipeline is complete, we will fit it to the indexed data. 


```{python}

assembleModel = pipeline.fit(indexed)


```


```{python}

final_data = assembleModel.transform(indexed).select(selectedCols)



```

```{python}


final_data.show(5)


```

Fitting the pipeline on our indexed data is a notable example of an estimator being fit to the data. The pipeline contains the estimators we created (stringIndexer, oheEncoder, assembler) and fit them to the indexed data. The result creates our model.


We then used assembleModel to transform the indexed data. The resulting final_data data frame now has a new column "features". Within the brackets of each row of the "features" column are all our predictor features combined, (assembled), into an array ([13,22,45,52, ...])



```{python}


for k, v in final_data.schema["features"].metadata["ml_attr"]["attrs"].items():
    final_features_df = pd.DataFrame(v)



```

Below is the output of our final features, giving us a look on how the one-hot coded features are now displayed.



```{python}


print(final_features_df.head(50))


```


### Scaling



Scaling helps ensure that the data is on the same scale. The purpose is to ensure that all features contribute equally to the model and to avoid the domination of features with larger values. We will be use the Min-Max method which re-scales features so they end up ranging between 0 and 1.




```{python}

scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")


```


```{python}

scalerModel = scaler.fit(final_data)


```

```{python}


scaled_data = scalerModel.transform(final_data)
final_data_scaled = scaled_data.select('label','scaledFeatures')

```

```{python}


scaled_data.show(5)

```

The scaled data output now has a new column 'scaledFeatures' containing our scaled features. The original 'features ' column still exists, as we can see from the output. For the final scaled data frame, we will change the name of 'scaledFeatures' back to 'features' and remove the previous 'features' column.


```{python}


final_data_scaled = final_data_scaled.withColumnRenamed("scaledFeatures","features")


```

```{python}


final_data_scaled.show(5)


```

Our label and features are now in the appropriate format for use in our classification models.


## Train Test Split

The final scaled data set will now be split into training and testing sets. First, our models will be fit (trained) on our training data. Then, our models will be evaluated on our test data so we can judge how well the characteristics of the training data generalize to new, unseen data. We will check if our model is overfit. Overfitting is when a model becomes so good on our training data that it has mastered every pattern. This makes the model perform well with training data but poorly with test (unseen) data.


```{python}


flights_train, flights_test = final_data_scaled.randomSplit([.80, .20], seed=43)


```


```{python}


training_ratio = flights_train.count() / final_data.count()



```



```{python}


training_ratio



```


```{python}


display(flights_train)


```


```{python}

display(flights_test)



```


```{python}


print("Training Set Count", flights_train.count())
print("Test Set Count", flights_test.count())


```

We will use the .reparation method to ensure Spark can parallelize tasks across all executor cpu cores.

```{python}



flights_train=flights_train.repartition(64)



```

Since we will be using the training data multiple times we will cache it, which will reduce re-computation across stages.


```{python}


flights_train.cache().count()


```

# Models

## Evaluation metrics

### Definitions




•	Accuracy-measures the number of predictions that are correct as a percentage of the total number of predictions that are made. As an example, if 90% of your predictions are correct, your accuracy is simply 90%. Calculation: number of correct predictions/Number of total predictions. TP+TN/(TP+TN+FP+FN)
•	Precision-tells us about the quality of positive predictions. It may not find all the positives but the ones that the model does classify as positive are likely to be correct. As an example, out of everyone predicted to have defaulted, how many of them did default? So, within everything that has been predicted as positive, precision counts the percentage that is correct. Calculation: True positives/All Positives. TP/(TP+FP)
•	Recall- tells us about how well the model identifies true positives. The model may find a lot of positives, yet it also will incorrectly detect many positives that are not actually positives. Out of all the patients who have the disease, how many were correctly identified? So, within everything that is positive, how many did the model successfully find? A model with low recall is unable to find all (or a large part) of the positive cases in the data. Calculated as: True Positives/ (False Negatives + True Positives)
•	F1 Score-The F1 score is defined as the harmonic mean of precision and recall. The formula for the F1 score is the following: 2 times(precision*Recall)/ (Precision + Recall)) Since the F1 score is an average of Precision and Recall, it means that the F1 score gives equal weight to Precision and Recall.

   


Precision-recall curve-A precision-recall curve is a plot that visualizes the trade-off between precision and recall at various thresholds for a binary classification model. A perfect model would have a curve that bends to the top-right corner, reaching a precision and recall of 1. The PR curve focuses on the performance of the model on the positive class, making it more relevant when detecting specific, important cases is the primary goal.

Precision-recall Area Under the Curve (PR AUC)-Calculates the area under the Precision-recall curve. A higher PR AUC indicates better performance, as it represents a better balance of precision and recall across different thresholds.




ROC Curve-gives a visual representation of the trade-offs between the true positive rate (TPR) and false positive rate (FPR) at various thresholds. It helps gain insights into how how well the model can balance the trade-offs between detecting positive instances and avoiding false positives across different thresholds.

Area Under the Curve Score (AUC)-evaluates a binary classification model's ability to distinguish between classes across various thresholds. The score, which ranges from 0 to 1, indicates how well the model's predicted probabilities separate positive from negative cases. When AUC is close to 1, our model is exhibiting excellent performance with a strong ability to distinguish between classes. An AUC around 0.5 shows that the model is not doing any better than random guessing, i.e a coin toss. An AUC score closer to 0 is an indicates the model is getting it entirely wrong, worse than a coin toss.

True positive rate (TPR)- Reflects a model’s ability to correctly identify positive instances. It measures the proportion of actual positive cases that the model successfully identifies. False positive rate (FPR)-Represents how often our model incorrectly classifies negative class instances as positive. It measures the proportion of actual negative instances that are incorrectly identified as positive by the model, indicating the rate of false predictions.

For our analysis, the negative class (flight arrives on time) is label 0. The positive class (flight arrives late) is label 1. Our event of interest are flights that arrive late. 



```{python}

# Binary evaluator (for ROC AUC)
binary_eval = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")




```


```{python}



# Multiclass metrics
evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
accuracy_eval = MulticlassClassificationEvaluator(metricName="accuracy")
precision_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="weightedPrecision")
recall_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="weightedRecall")
f1_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="f1")


```

```{python}


accuracy_eval = MulticlassClassificationEvaluator(metricName="accuracy")
evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")



```



```{python}


recall_test_eval=MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedRecall")
    
precision_test_eval=MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedPrecision")



```



## Logistic REgression



In simple terms Logistic Regression is about probabilities. It measures the likelihood of an event occurring. Logistic regression aims to find the probability that a given input belongs to a certain class. Our model has many inputs, thus depending on the combination of inputs our model will predict the probability of an event being '0' (flight arriving on time) or '1' (flight arriving late).


<br>

Instantiate the Logistic Regression model

```{python}



lr = LogisticRegression(labelCol="label",
                        featuresCol="features",
                        family='binomial')


```



```{python}


evaluator = BinaryClassificationEvaluator(labelCol='label',
                                          rawPredictionCol="rawPrediction",
                                          metricName='areaUnderROC')


```




K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets. For example, we will be setting the number of folds at 3 ( numFolds=3. K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses two-thirds of the data for training and one-third for testing. Each fold is used as the test set exactly once. In other words, each fold will act as a training set once while the other folds are part of the testing population.




```{python}

lr_cv = CrossValidator(estimator=lr,
                       estimatorParamMaps=lr_params,
                       evaluator=evaluator,
                       numFolds=3,
                       collectSubModels=True,
                       parallelism=8)



```




```{python}


lr_cv_model = lr_cv.fit(flights_train)


```



The fit method that we will use for our classification models is like how it was used in our pre-processing step for our String Indexer Model. For machine learning, the fit method teaches the model how to understand and interpret the data. We only use the fit method on the training data.



We will evaluate the cross validation by using the average metric, which is the average area under the ROC curve for each parameter combination.


```{python}

lr_avg_cv_metrics



```


```{python}


lr_avg_cv_metrics=lr_cv_model.avgMetrics[0]


```


```{python}

print(f"Logistic Regression Average Cross Validation Score:{lr_avg_cv_metrics:.2f}")



```




The best model is an estimate of the cross-validation’s performance. It provides a reliable estimate of how well a given model configuration (logistic regression algorithm and hyperparameters) will perform on unseen data. This is done by averaging performance metrics across multiple validation folds. 

```{python}

lr_model = lr_cv_model.bestModel


```



```{python}




print(lr_cv_model.bestModel)



```



```{python}


lr_train_summary = lr_model.summary



```



```{python}


# Print out the Area Under the ROC Curve (AUC)
print('Training Set AUC: {:.3f}'.format(log_train_summary.areaUnderROC))


```


```{python}



lr_train_preds = lr_model.transform(flights_train)


```

The transform method applies what was learned from fitting the training data and applies it to both the training and testing data. 



```{python}


lr_train_evaluator_accuracy = MulticlassClassificationEvaluator(
    metricName="accuracy")


```


```{python}


lr_train_accuracy = accuracy_eval.evaluate(lr_train_preds) * 100



```


```{python}



print("Logistic Regression Train Accuracy: {0:.2f}".format(lr_train_accuracy),
      "%")



```



The accuracy score of 92% is very good for our model though we will view other scoring metrics before making judgement on our model.

The ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left). The AUC score summarizes the ROC curve in a single number. AUC indicates how well a model performs across all values of the threshold. An ideal model that performs perfectly regardless of the threshold, would have AUC of 1. A score of one would be displayed on the ROC plot as the curve bends at the upper left corner of the plot.



```{python}

lr_train_precision_label_1=lr_train_summary.precisionByLabel[1]
lr_train_recall_label_1=lr_train_summary.recallByLabel[1]
lr_train_f1_label_1=lr_train_summary.fMeasureByLabel()[1]
print("Logistic Regressiont Train Precision Label 1: {0:.2f}".format(lr_train_precision_label_1))
print("Logistic Regression Train Recall Label 1: {0:.2f}".format(lr_train_recall_label_1))
print("Logistic Regression Train F1 Score Label 1: {0:.2f}".format(lr_train_f1_label_1))


```


```{python}

lr_train_precision_label_0=lr_train_summary.precisionByLabel[0]
lr_train_recall_label_0=lr_train_summary.recallByLabel[0]
lr_train_f1_label_0=lr_train_summary.fMeasureByLabel()[0]
print("Logistic Regressiont Train Precision Label 0: {0:.2f}".format(lr_train_precision_label_0))
print("Logistic Regression Train Recall Label 0: {0:.2f}".format(lr_train_recall_label_0))
print("Logistic Regression Train F1 Score Label 0: {0:.2f}".format(lr_train_f1_label_0))



```

The ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left). The AUC score summarizes the ROC curve in a single number. AUC indicates how well a model performs across all values of the threshold. An ideal model that performs perfectly regardless of the threshold, would have AUC of 1. A score of one would be displayed on the ROC plot as the curve bends at the upper left corner of the plot.



```{python}



print(" ")
print("Precision by Label:")
for i, prec in enumerate(log_train_summary.precisionByLabel):
    print("label %d: %s" % (i, prec))


```

We are evaluating our precision score by label (0=flight arrived on time, 1=flight did not arrive on time). The precision score for label 0 is 0.95 (rounded up) which tells us that this model is excellent ensuring predictions of label '0' are in fact label '0'. Another way of looking at it is 95% of labels predicted as '0' are correct. Precision score for label '1' is not as high with a score of .80. If we had to rate the model on correctly predicting label '1' we would rate it as 'good'. Looking at it another way, 80 % of labels predicted as '1' are correct, though this also means 20% of labels predicted as '1' are incorrect.



```{python}


print(" ")
print("Recall by Label:")
for i, prec in enumerate(log_train_summary.recallByLabel):
    print("label %d: %s" % (i, prec))



```



Now, we will evaluate the recall score by label. With a recall score of .96 for label '0', our model is excellent at finding this label. We can say that our model is able to find 96% of all possible labels that are '0'. The recall score for label '1' is .77, .19 lower than the score for label '0'. Our model is only able to find 77% of all possible labels that are '1'.
For this analysis, our event of interest is flights that arrive late (label 1). That is, we are interested in the predictions of flights that are late as that is the problem we are concerned with solving.
Like ROC curves, precision-recall curves provide a graphical representation of a classifier’s performance across many thresholds. The precision-recall curve shows the tradeoff between precision and recall for different thresholds. Also, just as the ROC curve has a quantitative measure so does the precision recall curve. This summary metric is the AUC-PR. AUC-PR stands for area under the (precision-recall) curve. The higher the AUC-PR score, the better a classifier performs for the given task. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning most of all positive results (high recall).





For this analysis, our event of interest is flights that arrive late (label 1). That is, we are interested in the predictions of flights that are late as that is the problem we are concerned with solving.




Like ROC curves, precision-recall curves provide a graphical representation of a classifier’s performance across many thresholds. The precision-recall curve shows the tradeoff between precision and recall for different thresholds. Also, just as the ROC curve has a quantitative measure so does the precision recall curve. This summary metric is the AUC-PR. AUC-PR stands for area under the (precision-recall) curve. The higher the AUC-PR score, the better a classifier performs for the given task. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning most of all positive results (high recall).




```{python}



lr_auprc_train = evaluator.evaluate(lr_train_preds,
                           {evaluator.metricName: "areaUnderPR"})
print("Logistic Regression Area Under Precision-Recall Curve Score-Training: {:.4f}".format(lr_auprc_train))



```


```{python}


pr = lr_train_summary.pr.toPandas()
plt.plot(pr['recall'], pr['precision'])
plt.ylabel('Precision')
plt.xlabel('Recall')
plt.title('Precision-Recall  Curve')

plt.show()

%matplotlib inline



```



```{python}


plt.clf()


```


Viewing the Precision Recall curve for the logistic regression model, we find the curve starts its bend at .95 precision and .62 recall. The area under the precision recall curve is .85, which gives our model an overall good score. Both the score and the precision recall curve reflect the differences between label '0' and label '1'.



```{python}


# Print out the Area Under the ROC Curve (AUC)
lr_train_auc=lr_train_summary.areaUnderROC
print('Logistic Regression Training Set AUC: {:.3f}'.format(lr_train_auc))



```



```{python}


trainingSummary = lr_model.summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'], roc['TPR'])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC Curve')
plt.show()
%matplotlib inline



```

```{python}

plt.clf()



```





The ROC curve for the logistic regression model bends at just below the 0.8 true positive rate. The AUC score for our model is 0.926 which translates into a particularly good model. Yet this does not seem to match our ROC curve, as such a high score would bend much further to the upper left-hand corner of the plot. One reason for this is though the true positive rate is just below .80, the false positive rate is low at .05. Thus, our model sacrifices on predicting true positives by keeping false positives predictions low.




```{python}


train_predictions_and_labels = lr_train_preds.select(
    "prediction", "label").rdd.map(
        lambda row: (float(row["prediction"]), float(row["label"])))

train_metrics = MulticlassMetrics(train_predictions_and_labels)

# Confusion Matrix for Train Set
train_confusion_matrix = train_metrics.confusionMatrix().toArray()



```

A confusion matrix gives a breakdown of the model predictions versus the known labels. The confusion matrix consists of four counts which are labelled as follows: - "positive" indicates a prediction of 1, while - "negative" indicates a prediction of 0 and - "true" corresponds to a correct prediction, while - "false" designates an incorrect prediction.



```{python}

plt.figure(figsize=(10, 5))
#plt.subplot(1, 2, 1)
sns.heatmap(train_confusion_matrix,
            annot=True,
            fmt=".0f",
            cmap="Blues",
            xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["Actual 0", "Actual 1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Train Set)")

plt.show()
%matplotlib inline



```


```{python}

plt.clf()

```


We can use the matrix for general calculations on our model. For label '0', of all labels predicted as '0', 95% were correct. On all labels predicted as label's', 80% were correct.

### Test Set


We will now fit the logistic regresion model to our test data.

```{python}


lr_test_pred = lr_model.transform(flights_test)


```




```{python}



lr_test_pred.select("label", "prediction", "probability", "features").show(10)



```



For our review of the test set metrics, we will compare them with the training set metrics. How well our logistic regression model will determine if there are significant differences between training and testing metrics. If the test set metrics are significantly lower than those from the training set, then we can conclude that the model is overfit.



```{python}


recall_test_eval=MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedRecall")


```

```{python}



lr_test_evaluator_accuracy = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="accuracy")


```


```{python}


lr_test_accuracy = accuracy_eval.evaluate(lr_test_pred)


```


```{python}


lr_test_pred_metrics = lr_model.evaluate(
    lr_test_pred.select("label", "features"))



```


```{python}



lr_test_evaluator_recall = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedRecall")



```


```{python}


lr_test_metrics = lr_test_evaluator_recall.evaluate(lr_test_pred)



```


```{python}



print("Logistic Regression Test Accuracy: {0:.2f}".format(lr_test_accuracy))


```


```{python}



lr_test_precision_label_1 = lr_test_pred_metrics.precisionByLabel[1]
lr_test_precision_label_0 = lr_test_pred_metrics.precisionByLabel[0]


```


```{python}


lr_test_recall_label_1 = lr_test_pred_metrics.recallByLabel[1]
lr_test_recall_label_0 = lr_test_pred_metrics.recallByLabel[0]


```


```{python}


lr_test_f1_label_1=lr_test_pred_metrics.fMeasureByLabel()[1]
lr_test_f1_label_0=lr_test_pred_metrics.fMeasureByLabel()[0]



```


```{python}


print("Logistic Regression Test Precion Label 1: {:.4f}".format(lr_test_precision_label_1))



```


```{python}


print("Logistic Regression Test Recall Label 1: {:.4f}".format(lr_test_recall_label_1))
print("Logistic Regression Test Recall Label 0: {:.4f}".format(lr_test_recall_label_0))



```


```{python}


lr_auroc_test = evaluator.evaluate(lr_test_pred, {evaluator.metricName: "areaUnderROC"})
lr_auprc_test = evaluator.evaluate(lr_test_pred, {evaluator.metricName: "areaUnderPR"})
print("Logistic Regression Area under ROC Curve-Test: {:.4f}".format(auroc_test))
print("Logistic Regression Area under PR Curve-Test: {:.4f}".format(auprc_test))





```


### Feature Importance


Important features, as the name suggests, are those features that are important in a model's predictions. We will perform analysis on which features hold the greatest weight for a model.



Before we calculate feature importance, we will calculate coefficients. For logistic regression. Coefficients help us determine how much a target feature moves based on a unit move of a predictor feature.


```{python}




coeff_array = lr_model.coefficients.toArray()


```



```{python}



coef_values=lr_model.coefficients.values



```



```{python}


coef_values




```


The above output displays the coefficients in an array with no features names.   We will write a function that will extract the coefficients along with the associated feature names. 


```{python}




def extract_feature_coef(coef, dataset, features_col):
    list_extract = []
    for i in dataset.schema[features_col].metadata["ml_attr"]["attrs"]:
        list_extract = list_extract + dataset.schema[features_col].metadata["ml_attr"]["attrs"][i]
    coefficients = pd.DataFrame(list_extract)
    coefficients['coef'] = coefficients['idx'].apply(lambda x: coef[x])
    return(coefficients.sort_values('coef', ascending = False))


```



```{python}


coef_features=extract_feature_coef(coeff_array,final_data,'features')



```



Features with a zero coefficient do not have an impact on our model so we will filter the coefficients to omit zero values. 


```{python}


coef_features=coef_features[coef_features['coef']!=0]




```

```{python}




print(coef_features.sort_values(["coef"]))


```


Logistic regression does not have a specific feature importance function as the other classification algorithms we will be using. As such, for us to determine features importance, the coefficients will be transformed to absolute values, meaning that negative values will  convert to positive values. Note that this is only done for feature importance and is not intended for traditional logistic regression analysis. 



```{python}


coef_features['abs_coef'] = coef_features["coef"].abs()


```


```{python}


features_df = coef_features


```




```{python}

features_df["Logistic Regression"] = features_df["abs_coef"]



```



```{python}


features_df.head(9)


```

Evaluating the logistic regression features we find that distance groups are most important in the prediction of the target features. All other features have small effects on our model.



```{python}


features_df.info()


```


## Random Forest

Random forest is an example of ensemble learning. Ensemble learning takes predictions from multiple models are merges them to enhance the accuracy of prediction. Random Forest models are made up of individual decision trees whose predictions are combined for a final result. The final result is decided using majority rules which means that the final prediction is what the majority of the decision tree models chose. A decision tree is a flowchart-like model that breaks down complex decisions into a series of simple questions or rules that follow a logical order. The answers to these questions or rules are either yes or no and based on the answer break off to another branch where another question awaits. Questions are asked to further split the data into uniform groups. This question-and-answer process continues until a decisive point is reached, which provides the final decision or prediction.



```{python}


rf = RandomForestClassifier(labelCol="label", featuresCol="features")


```


```{python}



evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',
                                          labelCol="label")

```



```{python}




rf_params = ParamGridBuilder() \
            .addGrid(rf.maxDepth, [2, 5, 10]) \
            .addGrid(rf.numTrees, [10, 20, 30]) \
            .addGrid(rf.maxBins, [5, 10 , 20]) \
            .addGrid(rf.minInfoGain, [0, 0.01 , 0.1]) \
            .build()



```

Create a cross-validator


```{python}


rf_cv = CrossValidator(estimator=rf,
                       estimatorParamMaps=rf_params,
                       evaluator=evaluator,
                       numFolds=3,
                       parallelism=8)




```


```{python}


rf_cv_model = rf_cv.fit(flights_train)



```

### Training Metrtics

```{python}

rf_cv_avg_metrics=rf_cv_model.avgMetrics[0]

```


```{python}


print(f"Average Random Forest Cross Validation Score: {rf_cv_avg_metrics:.2f}")


```

```{python}


rf_model = rf_cv_model.bestModel

```

```{python}

print(rf_cv_model.bestModel)


```

```{python}

rf_train_summary = rf_model.summary

```


```{python}

rf_train_preds = rf_model.transform(flights_train)

```

```{python}


rf_train_accuracy = (
    accuracy_eval.evaluate(rf_train_preds)) * 100


```




```{python}


accuracy_eval = BinaryClassificationEvaluator(metricName="accuracy")


```


```{python}

print("Random Forest Train Accuracy: {0:.2f}".format(rf_train_accuracy))


```

```{python}



rf_train_auroc = evaluator.evaluate(rf_train_preds,
                                    {evaluator.metricName: "areaUnderROC"})


```

```{python}


print("Random Forest Train Area under ROC Curve: {:.4f}".format(rf_train_auroc))


```

```{python}


trainingSummary = rf_model.summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'], roc['TPR'])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('Random Forest Training ROC Curve')
plt.show()
%matplotlib inline



```

```{python}



plt.clf()



```


```{python}


print(" ")
print("Precision by Label:")
for i, prec in enumerate(rf_train_summary.precisionByLabel):
    print("label %d: %s" % (i, prec))



```


```{python}

print(" ")
print("Recall by Label:")
for i, prec in enumerate(rf_train_summary.recallByLabel):
    print("label %d: %s" % (
        i,
        prec,
    ))



```


```{python}

rf_train_precision_label_1=rf_train_summary.precisionByLabel[1]
rf_train_recall_label_1=rf_train_summary.recallByLabel[1]
rf_train_f1_label_1=rf_train_summary.fMeasureByLabel()[1]
print("Random Forest Train Precision Label 1: {0:.2f}".format(rf_train_precision_label_1))
print("Random Forest Train Recall Label 1: {0:.2f}".format(rf_train_recall_label_1))
print("Random Forest Train F1 Score Label 1: {0:.2f}".format(rf_train_f1_label_1))


```

```{python}

rf_train_precision_label_0=rf_train_summary.precisionByLabel[0]
rf_train_recall_label_0=rf_train_summary.recallByLabel[0]
rf_train_f1_label_0=rf_train_summary.fMeasureByLabel()[0]
print("Random Forest Train Precision Label 0: {0:.2f}".format(rf_train_precision_label_0))
print("Random Forest Train Recall Label 0: {0:.2f}".format(rf_train_recall_label_0))
print("Random Forest Train F1 Score Label 0: {0:.2f}".format(rf_train_f1_label_0))



```


```{python}


rf_auprc_train = evaluator.evaluate(rf_train_preds,
                           {evaluator.metricName: "areaUnderPR"})
print("Random Forest Area Under Precision-Recall Curve Score-Training: {:.4f}".format(rf_auprc_train))


```


```{python}


pr = rf_train_summary.pr.toPandas()
plt.plot(pr['recall'], pr['precision'])
plt.ylabel('Precision')
plt.xlabel('Recall')
plt.title('Random Forest Training Precsion-Recall Curve')
plt.show()

%matplotlib inline


```


```{python}



plt.clf()


```

```{python}


rf_train_auroc = evaluator.evaluate(rf_train_preds,
                                    {evaluator.metricName: "areaUnderROC"})


```


```{python}


rint("Random Forest Train Area under ROC Curve: {:.4f}".format(rf_train_auroc))


```


```{python}

trainingSummary = rf_model.summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'], roc['TPR'])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('Random Forest Training ROC Curve')
plt.show()
%matplotlib inline



```


```{python}



plt.clf()


```


```{python}


train_predictions_and_labels = rf_train_preds.select(
    "prediction", "label").rdd.map(
        lambda row: (float(row["prediction"]), float(row["label"])))

train_metrics = MulticlassMetrics(train_predictions_and_labels)

# Confusion Matrix for Train Set
train_confusion_matrix = train_metrics.confusionMatrix().toArray()


```


```{python}



plt.figure(figsize=(10, 5))
#plt.subplot(1, 2, 1)
sns.heatmap(train_confusion_matrix,
            annot=True,
            fmt=".0f",
            cmap="Blues",
            xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["Actual 0", "Actual 1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Random Forest (Train Set)")

plt.show()

%matplotlib inline



```


```{python}

plt.clf()


```


### Test Set




```{python}


rf_test_pred = rf_model.transform(flights_test)


```


```{python}


rf_pred_summary = rf_test_pred.summary


```


```{python}


rf_test_pred.select("label", "prediction", "probability", "features").show(10)


```

### Test Metrics


```{python}


rf_test_accuracy = accuracy_eval.evaluate(rf_test_pred) * 100


```


```{python}


rf_test_auroc = evaluator.evaluate(rf_test_pred,
                                   {evaluator.metricName: "areaUnderROC"})


```


```{python}

rf_test_aupr = evaluator.evaluate(rf_test_pred,
                                   {evaluator.metricName: "areaUnderPR"})



```


```{python}


print("Random Forest Test Accuracy: {0:.2f}".format(rf_test_accuracy), "%")


```


```{python}


rf_test_metrics = rf_model.evaluate(
    rf_test_pred.select("label", "features"))



```


```{python}


rf_test_precision_label_1=rf_test_metrics.precisionByLabel[1]
rf_test_precision_label_0=rf_test_metrics.precisionByLabel[0]


```


```{python}


print( f"Random Forest Test Precision label 1: {rf_test_precision_label_1:.2f}")
print(f"Random Forest Test precision label 0: {rf_test_precision_label_0:.2f}")


```


```{python}


rf_test_recall_label_1=rf_test_metrics.recallByLabel[1]
rf_test_recall_label_0=rf_test_metrics.recallByLabel[0]


```



```{python}


print(f"Random Forest Test Recall label 1: {rf_test_recall_label_1:.2f}")
print(f"Random Forest Test Recall label 0: {rf_test_recall_label_0:.2f}")



```


```{python}


rf_test_f1_label_1=lr_test_pred_metrics.fMeasureByLabel()[1]
rf_test_f1_label_0=lr_test_pred_metrics.fMeasureByLabel()[0]


```


```{python}


print( f"Random Forest Test F1 label 1: {rf_test_f1_label_1:.2f}")
print(f"Random Forest Test F1 label 0: {rf_test_f1_label_0:.2f}")


```


```{python}


print("Random Forest Test Area under ROC Curve: {:.3f}".format(rf_test_auroc)) 
print("Random Forest Test Area under PR Curve: {:.3f}".format(rf_test_aupr))


```

### Feature Importance


```{python}


def extract_feature_imp(feature_imp, dataset, features_col):
    list_extract = []
    for i in dataset.schema[features_col].metadata["ml_attr"]["attrs"]:
        list_extract = list_extract + dataset.schema[features_col].metadata[
            "ml_attr"]["attrs"][i]
    feature_list = pd.DataFrame(list_extract)
    feature_list['score'] = feature_list['idx'].apply(lambda x: feature_imp[x])
    return (feature_list.sort_values('score', ascending=False))


```


```{python}



rf_featureImportances = rf_model.featureImportances.toArray()



```



```{python}


feature_list = extract_feature_imp(rf_featureImportances, final_data,
                                   "features")
top_20_features = feature_list.sort_values('score', ascending=False).head(20)


```



```{python}



plt.figure(figsize=(10,7))

sns.barplot(x='score', y='name', data=top_20_features)
plt.title("Random Forest Important Features")
plt.tick_params(axis='y', which='major', labelsize=7)
plt.ylabel(None)

plt.show()
%matplotlib inline

```



```{python}


plt.clf()



```


```{python}





```



```{python}





```





