---
title: "Flight Delay Prediction"
author: "Frank Laudert"
date: "2025-10-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


For the  first part of our flight arrival project, we cleaned, prepared and explored the combined flight data sets. (https://franklauder.github.io/Flight_Delays/flightAnalysis.html) For the second part of our project, we are ready to move on to predict whether flights will be on time or later. Our focus is on flights arriving late. For predicting flight arrivals, we will use three classifier algorithms (logistic regression, random forest, and gradient boost). The data sets were downloaded from Kaggle. They can be viewed by going to the following link: https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018


# Libraries

```{python}




import pyspark
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.ml.linalg import Vectors

from pyspark.ml.param import Param, Params
from pyspark.sql.functions import col, desc
from pyspark.sql.types import StringType,BooleanType,IntegerType,DoubleType,StructType,StructField,LongType,ShortType

from pyspark.ml.feature import VectorAssembler,MinMaxScaler, StandardScaler

from pyspark.ml import Pipeline

from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics

from pyspark.mllib.stat import Statistics
from pyspark.ml.feature import StringIndexer,OneHotEncoder


from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,GBTClassifier,RandomForestClassifier,LinearSVC


from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator


import pyspark.ml.evaluation as evals

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,CrossValidatorModel

from pyspark.ml.stat import Correlation

from pyspark.sql.functions import *

from pyspark.sql.window import *
from pyspark.sql.functions import row_number

import itertools
from pyspark.sql import SQLContext

from pyspark import SparkContext




```



```{python}




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import seaborn as sns


```


```{python}



from sklearn.metrics import roc_curve, precision_recall_curve, auc


```

```{python}



pd.set_option("display.max_rows", None)

```


```{python}


from pyspark import SparkContext


```


```{python}



print(spark)
print(sc)


```



```{python}



spark.conf.set("spark.sql.adaptive.enabled", "true")


```

# Data

## Data Dictionary


OP_CARRIER-Airline carrier code

DEP_DELAY-Difference in minutes between scheduled and actual departure time. Early departures show negative numbers.

AXI_OUT-The time elapsed between departure from the origin airport gate and wheels off, in minutes.

TAXI_IN-Taxi time from wheels down to arrival at the gate, in minutes. CARRIER_DELAY-Carrier Delay, in Minutes. Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.

WEATHER_DELAY-Weather Delay, in Minutes. Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.

NAS_DELAY-National Air System Delay, in Minutes.Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc. Delays that occur after Actual Gate Out are usually attributed to the NAS and are also reported through OPSNET.

SECURITY_DELAY-Security Delay, in Minutes. Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.

LATE_AIRCRAFT_DELAY-Late Aircraft Delay, in Minutes. Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation.

schDepWhOffDiff-Difference between scheduled departure time and wheels up.

wheOnScArrDiff-Difference between wheels down and scheduled arrival time.

DayOfWeek-1 (Monday) - 7 (Sunday)

Month-1-12

DepartureDelayGroups-Departure Delay intervals, every (15 minutes from <-15 to >180). Negative observations are set to zero.

0 Delay between 0 and 14 minutes 1 Delay between 15 to 29 minutes 2 Delay between 30 to 44 minutes 3 Delay between 45 to 59 minutes 4 Delay between 60 to 74 minutes 5 Delay between 75 to 89 minutes 6 Delay between 90 to 104 minutes 7 Delay between 105 to 119 minutes 8 Delay between 120 to 134 minutes 9 Delay between 135 to 149 minutes 10 Delay between 150 to 164 minutes 11 Delay between 165 to 179 minutes 12 Delay >= 180 minutes

DepTimeBlk-CRS Departure Time Block, Hourly Intervals

1--001-0559 12:00AM to 5:59AM 2--0600-0659 6:00AM to 6:59AM 3--0700-0759 7:00AM to 7:59AM 4--0800-0859 8:00AM to 8:59AM 5--0900-0959 9:00AM to 9:59AM 6--1000-1059 10:00AM to 10:59AM 7--1100-1159 11:00AM to 11:59AM 8--1200-1259 12:00PM to 12:59PM 9--1300-1359 1:00PM to 1:59PM 10--1400-1459 2:00PM to 2:59PM 11--1500-1559 3:00PM to 3:59PM 12--1600-1659 4:00PM to 4:59PM 13--1700-1759 5:00PM to 5:59PM 14--1800-1859 6:00PM to 6:59PM 15--1900-1959 7:00PM to 7:59PM 16--2000-2059 8:00PM to 8:59PM 17--2100-2159 9:00PM to 9:59PM 18--2200-2259 10:00PM to 10:59PM 19--2300-2359 11:00PM to 11:59PM

DistanceGroup-Distance Intervals, every 250 Miles, for Flight Segment

1--Less Than 250 Miles 2--250-499 Miles 3--500-749 Miles 4--750-999 Miles 5--1000-1249 Miles 6--1250-1499 Miles 7--1500-1749 Miles 8--1750-1999 Miles 9--2000-2249 Miles 10--2250-2499 Miles 11--500 Miles and Greater



## Import & Review

We will import our cleaned flight data from its stored location in a Amazon Web Services S3 bucket

```{python}


flight_10=spark.read.format('csv').options(header='true', inferSchema='true').load('s3://databricks-workspacedemo-cloud-storage-bucket/DBWorkData/csv3/flight_10/')


```



```{python}



flight_10.show()


```




```{python}


display(flight_10)


```



```{python}


flight_10.count()


```



```{python}

flight_10.columns


```



```{python}


print(flight_10.dtypes)


```


```{python}

flight_10.printSchema()

```




Our review of the flight_10 dataset reveals it has nineteen columns and a little over six million rows. 

We will transform certain features from numeric data types to categorical data data types.


```{python}


flight_11= flight_10.withColumn("DepTimeBlk",col("DepTimeBlk").cast(StringType())) \
            .withColumn("Month",col("Month").cast(StringType())) \
            .withColumn('DayofMonth',col('DayofMonth').cast(StringType())) \
            .withColumn('DayOfWeek',col('DayOfWeek').cast(StringType())) \
            .withColumn("ArrDel15",col("ArrDel15").cast(StringType())) \
            .withColumn("DepartureDelayGroups",col("DepartureDelayGroups").cast(StringType())) \
            .withColumn("DistanceGroup",col("DepartureDelayGroups").cast(StringType()))


```



```{python}



flight_11.printSchema() 


```


# Feature Selection

## Numeric Features


Correlation will be used to determine if there are mutual relationships between our numeric features.


First, we will extract numeric features from the data set. 

```{python}


numeric_cols = [
    c for c, t in flight_11.dtypes if t.startswith('string') == False
]


```




```{python}



numeric_cols = [
    c for c, t in flight_11.dtypes if t.startswith('string') == False
]

```


```{python}


print(numeric_df.dtypes)


```

Next, the VectorAssembler function will be used to transform the numeric features into a single column (Vector). VectorAssembler will appear again during the building of our pipeline.

```{python}


num_assembler = VectorAssembler(inputCols=numeric_cols,
                                outputCol="num_features")


```



```{python}


df_num_assembler = num_assembler.transform(numeric_df)


```



```{python}


df_num_assembler.show(3)

```


```{python}


or k, v in df_num_assembler.schema["num_features"].metadata["ml_attr"][
        "attrs"].items():
    features_df = pd.DataFrame(v)
column_names = list(features_df['name'])


```




```{python}


spark.createDataFrame(features_df).show(11)


```


```{python}


features_df.info()


```



```{python}


df_vector = df_num_assembler.rdd.map(lambda x: x['num_features'].toArray())


```


```{python}

df_vector.take(4)



```





```{python}


correlation_type = 'pearson'


```


```{python}


matrix = Statistics.corr(df_vector, method=correlation_type)

```



```{python}


corr_df = pd.DataFrame(matrix, columns=column_names, index=column_names)


```




```{python}


final_corr_df = pd.DataFrame(corr_df.abs().unstack().sort_values(kind='quicksort')).reset_index()
final_corr_df.rename({'level_0': 'col1', 'level_1': 'col2', 0: 'correlation_value'}, axis=1, inplace=True)
final_corr_df = final_corr_df[final_corr_df['col1'] != final_corr_df['col2']]


```

We are setting our correlation cutoff at .70. This means if any pair of features show a relationship of .70 or greater than one feature will be dropped from the data set.

```{python}


correlation_cutoff = 0.70 #custom parameter
final_corr_df[final_corr_df['correlation_value'] > correlation_cutoff]


```

Our result came back as an empty data frame. This means there are no features that meet our correlation cutoff. All numeric features will remain in our data set.



## Categorical Features


We will drop the feature 'DayofMonth' from the data set as it duplicates other features. 



```{python}



flight_12=flight_11.drop('DayofMonth')


```




```{python}


flight_12.printSchema()



```

# Pre-processing

## Sampling


```{python}


flight_12.count()


```

```{python}

flight_12_percent=flight_12.groupBy("ArrDel15").count().withColumn("Percent",col("count")/flight_12.count()*100)
flight_12_percent.show()

```

From our inspection of our data earlier, we observed that our data has just over sixty million rows. This size will take large resources and time to complete our classification models. Thus, we will sample our flight data to get a smaller representation that will use less resources and take less time to process.




```{python}

flight_sample = flight_12.sample(False, 0.3, 11)


```

```{python}


flight_sample.count()


```


```{python}


arrDelay_percent=flight_sample.groupBy("ArrDel15").count().withColumn("Percent",col("count")/flight_sample.count()*100)
arrDelay_percent.show()


```


Our sampled data is now just over 18 million rows. Additionally, the target feature's lables (0,1) are the same percentage as the original flight_12 data. 



## Pipeline

A Pipeline is simply a sequence of stages, each of which is either an Estimator or a Transformer. Each stage must be executed in order. An estimator is an object that fits a model based on the input data. Essentially, an estimator is an algorithm which can be fitted on a data frame to produce a transformer. A transformer is an algorithm which can transform one data frame into another data frame. In other words, a transformer takes a data frame with features and produces a data frame with additional features.

### Categorical Features

The first steps are to transform our categorical features. First, we transform our target feature ArrDel15 and rename it to "label" This will is accomplished by using StringIndexer. StringIndexer helps convert categorical string columns in a Data Frame into numerical indices. This conversion is necessary because most machine learning algorithms cannot work directly with string data. StringIndexer converts categorical string columns by assigning a unique index to each distinct string value in the input column and maps it to a new output column of integer indices.



```{python}

renamed=flight_sample.withColumn('label_str',flight_sample['ArrDel15'].cast(StringType()))

indexer=StringIndexer(inputCol='label_str',outputCol='label')


```

The .fit method is used to scan the input column to find unique string values present. The fit method then creates a mapping (think lookup table) where each unique string value is assigned a numerical index. It then returns a StringIndexerModel. 

Once we have our StringIndexer model we use the transform method to apply the learned mapping from String Indexer Model to the original data and create a new column (outputCol in the code). The outputCol contains the numerical indices for each value in the input column. 




```{python}

indexed = indexer.fit(renamed).transform(renamed)


```

```{python}


indexed.printSchema()


```

From the schema we find that the renamed target feature "label" now appears. We no longer require the original target feature name of "ArrDel15" and the "label_str" feature.


```{python}

indexed=indexed.drop("ArrDel15",'label_str')


```


```{python}


indexed.printSchema()


```

Next, predictor features will be transformed by the StringIndexer function. First the categorical columns will be extracted. The following code will pull all string data from the indexed data set, exclude the’ label’ feature, and create a list of categorical columns named categoricalCols.



```{python}


categoricalCols = [field for (field, dataType) in indexed.dtypes 
                   if dataType == "string"
                  and field != 'label']
indexOutputCols = [x + "Index" for x in categoricalCols]

```


```{python}

type(categoricalCols)


```


```{python}

categoricalCols



```




```{python}

stringIndexer = StringIndexer(inputCols=categoricalCols,
                              outputCols=indexOutputCols,
                              handleInvalid="skip")



```



The StringIndexer function takes in as input our categorical columns and returns as output the same categorical columns only now indexed. 



```{python}



indexOutputCols


```




We see that the indexOutPutCols string shows all categorical features as indexed.

Many machine learning algorithms require all the input and output variables to be numeric. For our purposes this requires all strings data types to be transformed into numeric data types. We will accomplish this by Appling one-hot-encoding to a string features. One-hot encoding will create new columns as much as the number of unique instances there are for a feature. If you have a feature 'performance' with three levels ('good', 'fair', 'poor'), the new columns will be filled with 0s and 1s. Our data frame now has three new features, 'performance.good', 'performanc.fair', and 'performance.poor'. This is considered creating dummy features.




```{python}

oheOutputCols = [x + "OHE" for x in categoricalCols]



```


```{python}



oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)


```

We now will extract our numerical columns from the indexed data. 



```{python}



oheEncoder


```

### Numerical Features

Numerical features, excluding the label, will be extracted from the indexed data frame. 





```{python}


numericCols = [
    field for (field, dataType) in indexed.dtypes
    if ((dataType == "int") & (field != "label"))
]



```

### Assemble


Categorcal and Numerical features lists will be combined for one features list.

```{python}


feature_list = oheOutputCols + numericCols



```


VectorAssembler will now be used to to combine the categorical and numeric list into one vector column.


```{python}

assembler = VectorAssembler(inputCols=feature_list, outputCol="features")


```


```{python}

All transformers used will be included in one list called stages.


```

```{python}

stages = [stringIndexer, oheEncoder, assembler]


```

```{python}

selectedCols = ['label', 'features'] + feature_list


```

Now that our pipeline is complete, we will fit it to the indexed data. 


```{python}

assembleModel = pipeline.fit(indexed)


```


```{python}

final_data = assembleModel.transform(indexed).select(selectedCols)



```

```{python}


final_data.show(5)


```

Fitting the pipeline on our indexed data is a notable example of an estimator being fit to the data. The pipeline contains the estimators we created (stringIndexer, oheEncoder, assembler) and fit them to the indexed data. The result creates our model.


We then used assembleModel to transform the indexed data. The resulting final_data data frame now has a new column "features". Within the brackets of each row of the "features" column are all our predictor features combined, (assembled), into an array ([13,22,45,52, ...])



```{python}


for k, v in final_data.schema["features"].metadata["ml_attr"]["attrs"].items():
    final_features_df = pd.DataFrame(v)



```

Below is the output of our final features, giving us a look on how the one-hot coded features are now displayed.



```{python}


print(final_features_df.head(50))


```


### Scaling



Scaling helps ensure that the data is on the same scale. The purpose is to ensure that all features contribute equally to the model and to avoid the domination of features with larger values. We will be use the Min-Max method which re-scales features so they end up ranging between 0 and 1.




```{python}

scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")


```


```{python}

scalerModel = scaler.fit(final_data)


```

```{python}


scaled_data = scalerModel.transform(final_data)
final_data_scaled = scaled_data.select('label','scaledFeatures')

```

```{python}


scaled_data.show(5)

```

The scaled data output now has a new column 'scaledFeatures' containing our scaled features. The original 'features ' column still exists, as we can see from the output. For the final scaled data frame, we will change the name of 'scaledFeatures' back to 'features' and remove the previous 'features' column.


```{python}


final_data_scaled = final_data_scaled.withColumnRenamed("scaledFeatures","features")


```

```{python}


final_data_scaled.show(5)


```

Our label and features are now in the appropriate format for use in our classification models.


## Train Test Split

The final scaled data set will now be split into training and testing sets. First, our models will be fit (trained) on our training data. Then, our models will be evaluated on our test data so we can judge how well the characteristics of the training data generalize to new, unseen data. We will check if our model is overfit. Overfitting is when a model becomes so good on our training data that it has mastered every pattern. This makes the model perform well with training data but poorly with test (unseen) data.


```{python}


flights_train, flights_test = final_data_scaled.randomSplit([.80, .20], seed=43)


```


```{python}


training_ratio = flights_train.count() / final_data.count()



```



```{python}


training_ratio



```


```{python}


display(flights_train)


```


```{python}

display(flights_test)



```


```{python}


print("Training Set Count", flights_train.count())
print("Test Set Count", flights_test.count())


```

We will use the .reparation method to ensure Spark can parallelize tasks across all executor cpu cores.

```{python}



flights_train=flights_train.repartition(64)



```

Since we will be using the training data multiple times we will cache it, which will reduce re-computation across stages.


```{python}


flights_train.cache().count()


```

# Models

## Evaluation metrics

### Definitions




•	Accuracy-measures the number of predictions that are correct as a percentage of the total number of predictions that are made. As an example, if 90% of your predictions are correct, your accuracy is simply 90%. Calculation: number of correct predictions/Number of total predictions. TP+TN/(TP+TN+FP+FN)
•	Precision-tells us about the quality of positive predictions. It may not find all the positives but the ones that the model does classify as positive are likely to be correct. As an example, out of everyone predicted to have defaulted, how many of them did default? So, within everything that has been predicted as positive, precision counts the percentage that is correct. Calculation: True positives/All Positives. TP/(TP+FP)
•	Recall- tells us about how well the model identifies true positives. The model may find a lot of positives, yet it also will incorrectly detect many positives that are not actually positives. Out of all the patients who have the disease, how many were correctly identified? So, within everything that is positive, how many did the model successfully find? A model with low recall is unable to find all (or a large part) of the positive cases in the data. Calculated as: True Positives/ (False Negatives + True Positives)
•	F1 Score-The F1 score is defined as the harmonic mean of precision and recall. The formula for the F1 score is the following: 2 times(precision*Recall)/ (Precision + Recall)) Since the F1 score is an average of Precision and Recall, it means that the F1 score gives equal weight to Precision and Recall.

   


Precision-recall curve-A precision-recall curve is a plot that visualizes the trade-off between precision and recall at various thresholds for a binary classification model. A perfect model would have a curve that bends to the top-right corner, reaching a precision and recall of 1. The PR curve focuses on the performance of the model on the positive class, making it more relevant when detecting specific, important cases is the primary goal.

Precision-recall Area Under the Curve (PR AUC)-Calculates the area under the Precision-recall curve. A higher PR AUC indicates better performance, as it represents a better balance of precision and recall across different thresholds.




ROC Curve-gives a visual representation of the trade-offs between the true positive rate (TPR) and false positive rate (FPR) at various thresholds. It helps gain insights into how how well the model can balance the trade-offs between detecting positive instances and avoiding false positives across different thresholds.

Area Under the Curve Score (AUC)-evaluates a binary classification model's ability to distinguish between classes across various thresholds. The score, which ranges from 0 to 1, indicates how well the model's predicted probabilities separate positive from negative cases. When AUC is close to 1, our model is exhibiting excellent performance with a strong ability to distinguish between classes. An AUC around 0.5 shows that the model is not doing any better than random guessing, i.e a coin toss. An AUC score closer to 0 is an indicates the model is getting it entirely wrong, worse than a coin toss.

True positive rate (TPR)- Reflects a model’s ability to correctly identify positive instances. It measures the proportion of actual positive cases that the model successfully identifies. False positive rate (FPR)-Represents how often our model incorrectly classifies negative class instances as positive. It measures the proportion of actual negative instances that are incorrectly identified as positive by the model, indicating the rate of false predictions.

For our analysis, the negative class (flight arrives on time) is label 0. The positive class (flight arrives late) is label 1. Our event of interest are flights that arrive late. 



```{python}

# Binary evaluator (for ROC AUC)
binary_eval = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")




```


```{python}



# Multiclass metrics
evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
accuracy_eval = MulticlassClassificationEvaluator(metricName="accuracy")
precision_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="weightedPrecision")
recall_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="weightedRecall")
f1_eval = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction",metricName="f1")


```

```{python}


accuracy_eval = MulticlassClassificationEvaluator(metricName="accuracy")
evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")



```



```{python}


recall_test_eval=MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedRecall")
    
precision_test_eval=MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="weightedPrecision")



```



## Logistic REgression



In simple terms Logistic Regression is about probabilities. It measures the likelihood of an event occurring. Logistic regression aims to find the probability that a given input belongs to a certain class. Our model has many inputs, thus depending on the combination of inputs our model will predict the probability of an event being '0' (flight arriving on time) or '1' (flight arriving late).


<br>

Instantiate the Logistic Regression model

```{python}



lr = LogisticRegression(labelCol="label",
                        featuresCol="features",
                        family='binomial')


```



```{python}


evaluator = BinaryClassificationEvaluator(labelCol='label',
                                          rawPredictionCol="rawPrediction",
                                          metricName='areaUnderROC')


```




K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets. For example, we will be setting the number of folds at 3 ( numFolds=3. K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses two-thirds of the data for training and one-third for testing. Each fold is used as the test set exactly once. In other words, each fold will act as a training set once while the other folds are part of the testing population.




```{python}

lr_cv = CrossValidator(estimator=lr,
                       estimatorParamMaps=lr_params,
                       evaluator=evaluator,
                       numFolds=3,
                       collectSubModels=True,
                       parallelism=8)



```




```{python}


lr_cv_model = lr_cv.fit(flights_train)


```



The fit method that we will use for our classification models is like how it was used in our pre-processing step for our String Indexer Model. For machine learning, the fit method teaches the model how to understand and interpret the data. We only use the fit method on the training data.



We will evaluate the cross validation by using the average metric, which is the average area under the ROC curve for each parameter combination.


```{python}

lr_avg_cv_metrics



```


```{python}


lr_avg_cv_metrics=lr_cv_model.avgMetrics[0]


```


```{python}

print(f"Logistic Regression Average Cross Validation Score:{lr_avg_cv_metrics:.2f}")



```




